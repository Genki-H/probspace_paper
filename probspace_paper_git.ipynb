{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitted-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "yellow-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage as gcs\n",
    "import io\n",
    "from io import BytesIO\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"hogehoge_project\" #プロジェクト名\n",
    "bucket_name = \"hogehoge_bucket\" #データを格納しているバケット名\n",
    "folder_path = \"hogehoge_folder/\" #データのあるフォルダパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elder-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = gcs.Client(project_name)\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vietnamese-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instrumental-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord の再定義\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 句読点の追加。string.punctuation = ['!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~']\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# 手動で追加\n",
    "org_stop = [\"Subject\"]\n",
    "\n",
    "# stopwordsの定義更新\n",
    "add_stop = punctuation + org_stop\n",
    "stop.update(add_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "romance-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# htmlの分割\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# []で囲まれた文章の削除（脚注、linkなど）\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# URLの削除\n",
    "def remove_URL(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# stopwordsの削除\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            if i.strip().isalpha():\n",
    "                final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "# 上記の関数をまとめて適用する関数を定義\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olive-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バージョン情報から論文の更新情報を抽出する関数を定義\n",
    "def preprocess(input_df):\n",
    "    output_df = input_df.copy()\n",
    "\n",
    "    output_df[\"first_created_unixtime\"] = pd.to_datetime(input_df.versions.apply(lambda p: p[0][\"created\"])).astype(int) / 1e9\n",
    "    output_df[\"last_created_unixtime\"] = pd.to_datetime(input_df.versions.apply(lambda p: p[-1][\"created\"])).astype(int) / 1e9\n",
    "    output_df[\"diff_created_unixtime\"] = output_df[\"last_created_unixtime\"] - output_df[\"first_created_unixtime\"]\n",
    "    output_df[\"num_created\"] = input_df.versions.apply(lambda p: len(p))\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accessible-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言語データをベクトル化するためのインスタンスを作成\n",
    "train_path = folder_path + \"GoogleNews-vectors-negative300.bin\"\n",
    "blob = gcs.Blob(train_path, bucket)\n",
    "content = blob.download_as_string()\n",
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format(BytesIO(content), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータをベクトル化する関数を定義　※平均\n",
    "def get_text_emb(emb_model, text):\n",
    "    tokens = text.split(\" \")\n",
    "    embs = []\n",
    "    for token in tokens:\n",
    "        emb = None\n",
    "        try:\n",
    "            emb = emb_model.get_vector(token)\n",
    "        except KeyError:\n",
    "            # 小文字化した単語はvocabularyに含まれているかもしれない\n",
    "            try:\n",
    "                emb = emb_model.get_vector(token.lower())\n",
    "            except KeyError:\n",
    "                emb = np.zeros(300)\n",
    "        if emb is None:\n",
    "            raise RuntimeError(\"emb is none\")\n",
    "        embs.append(emb)\n",
    "    mean_embs = np.mean(embs, axis=0)\n",
    "    return mean_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fitted-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータをベクトル化する関数を定義　※最大\n",
    "def get_text_emb_max(emb_model, text):\n",
    "    tokens = text.split(\" \")\n",
    "    embs = []\n",
    "    for token in tokens:\n",
    "        emb = None\n",
    "        try:\n",
    "            emb = emb_model.get_vector(token)\n",
    "        except KeyError:\n",
    "            # 小文字化した単語はvocabularyに含まれているかもしれない\n",
    "            try:\n",
    "                emb = emb_model.get_vector(token.lower())\n",
    "            except KeyError:\n",
    "                emb = np.zeros(300)\n",
    "        if emb is None:\n",
    "            raise RuntimeError(\"emb is none\")\n",
    "        embs.append(emb)\n",
    "    mean_embs = np.max(embs, axis=0)\n",
    "    return mean_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "divided-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種の定数を定義\n",
    "NFOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "together-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonファイルの読み込み\n",
    "def get_json_from_gcp(file_name):\n",
    "    train_path = folder_path + file_name\n",
    "    blob = gcs.Blob(train_path, bucket)\n",
    "    content = blob.download_as_string()\n",
    "    df = pd.read_json(BytesIO(content), lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-crash",
   "metadata": {},
   "source": [
    "## start point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_json_from_gcp(\"train_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "revised-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_json_from_gcp(\"test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extraordinary-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データで「cites」が欠損しているデータを削除\n",
    "train_df = train_df.dropna(subset=['cites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "metallic-cinema",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#　categoriesラベルエンコーディング\n",
    "t_all_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "statutory-beaver",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t_all_df[\"categories\"].fillna(\"missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unavailable-medicare",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le = le.fit(t_all_df[\"categories\"])\n",
    "t_all_df[\"categories\"] = le.transform(t_all_df[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "north-phoenix",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  doiをリスト抽出しラベルエンコーディング\n",
    "t_l_jnl = t_all_df[\"doi\"].values.tolist()\n",
    "\n",
    "tmp = []\n",
    "for i in t_l_jnl:\n",
    "    x = i.find(\"/\")\n",
    "    tmp.append(i[0:x])\n",
    "\n",
    "t_doi_df = pd.Series(tmp)\n",
    "t_all_df[\"s_doi\"] = t_doi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "animal-bench",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le = le.fit(t_all_df[\"s_doi\"])\n",
    "t_all_df[\"s_doi\"] = le.transform(t_all_df[\"s_doi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "powered-coffee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df = t_all_df[~t_all_df[\"cites\"].isnull()].reset_index(drop=True)\n",
    "test_df = t_all_df[t_all_df[\"cites\"].isnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "british-words",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データとテストデータから特長量を作成し格納するリストを作成\n",
    "train = []\n",
    "train_2 = []\n",
    "train_3 = []\n",
    "train_4 = []\n",
    "train_5 = []\n",
    "train_6 = []\n",
    "train_7 = []\n",
    "train_8 = []\n",
    "train_feat = []\n",
    "target = []\n",
    "\n",
    "test = []\n",
    "test_2 = []\n",
    "test_3 = []\n",
    "test_4 = []\n",
    "test_5 = []\n",
    "test_6 = []\n",
    "test_7 = []\n",
    "test_8 = []\n",
    "test_feat = []\n",
    "test_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mysterious-authorization",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「abstract」をベクトル化　※平均\n",
    "for abstract in train_df[\"abstract\"]:\n",
    "    abstract = denoise_text(abstract)\n",
    "    mean_embs = get_text_emb(emb_model, abstract)\n",
    "    train.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "iraqi-garage",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「abstract」をベクトル化　※最大\n",
    "for abstract in train_df[\"abstract\"]:\n",
    "    abstract = denoise_text(abstract)\n",
    "    mean_embs = get_text_emb_max(emb_model, abstract)\n",
    "    train_8.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ranking-retention",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「doi_cites」の対数を取る\n",
    "for doi_cites in train_df[\"doi_cites\"]:\n",
    "    log_doi_cites = [np.log1p(int(doi_cites))]\n",
    "    train_feat.append(log_doi_cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "convertible-cincinnati",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの目的変数「cites」を格納\n",
    "for cites in train_df[\"cites\"]:\n",
    "    target.append(cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "statutory-circuit",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「title」をベクトル化 ※平均\n",
    "for title in train_df[\"title\"]:\n",
    "    title = denoise_text(title)\n",
    "    mean_embs = get_text_emb(emb_model, title)\n",
    "    train_2.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "physical-auckland",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「abstract」をベクトル化 ※平均\n",
    "for abstract in test_df[\"abstract\"]:\n",
    "    abstract = denoise_text(abstract)\n",
    "    mean_embs = get_text_emb(emb_model, abstract)\n",
    "    test.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adverse-contrary",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「abstract」をベクトル化 ※最大\n",
    "for abstract in test_df[\"abstract\"]:\n",
    "    abstract = denoise_text(abstract)\n",
    "    mean_embs = get_text_emb_max(emb_model, abstract)\n",
    "    test_8.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "civilian-florist",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「doi_cites」の対数を取る\n",
    "for doi_cites in test_df[\"doi_cites\"]:\n",
    "    log_doi_cites = [np.log1p(int(doi_cites))]\n",
    "    test_feat.append(log_doi_cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "careful-juvenile",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「id」を格納\n",
    "for id in test_df[\"id\"]:\n",
    "    test_index.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "palestinian-basics",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「title」をベクトル化 ※平均\n",
    "for title in test_df[\"title\"]:\n",
    "    title = denoise_text(title)\n",
    "    mean_embs = get_text_emb(emb_model, title)\n",
    "    test_2.append(mean_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dangerous-rocket",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「cat」を格納\n",
    "for cat in train_df[\"categories\"]:\n",
    "    train_3.append(cat)\n",
    "\n",
    "tmp = np.array(train_3)\n",
    "train_3 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changed-transcription",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「cat」を格納\n",
    "for cat in test_df[\"categories\"]:\n",
    "    test_3.append(cat)\n",
    "\n",
    "tmp = np.array(test_3)\n",
    "test_3 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "junior-endorsement",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データの「s_doi」を格納\n",
    "for s_doi in train_df[\"s_doi\"]:\n",
    "    train_4.append(s_doi)\n",
    "\n",
    "tmp = np.array(train_4)\n",
    "train_4 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "intended-detective",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの「s_doi」を格納\n",
    "for s_doi in test_df[\"s_doi\"]:\n",
    "    test_4.append(s_doi)\n",
    "\n",
    "tmp = np.array(test_4)\n",
    "test_4 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "white-possession",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#　訓練データの更新情報を取得して、特長量として格納\n",
    "train_pre_df = preprocess(train_df)\n",
    "train_pre_tmp_df = train_pre_df.loc[:, [\"first_created_unixtime\", \"last_created_unixtime\", \"diff_created_unixtime\", \"num_created\"]]\n",
    "train_5 = train_pre_tmp_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "whole-excess",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#　テストデータの更新情報を取得して、特長量として格納\n",
    "test_pre_df = preprocess(test_df)\n",
    "test_pre_tmp_df = test_pre_df.loc[:, [\"first_created_unixtime\", \"last_created_unixtime\", \"diff_created_unixtime\", \"num_created\"]]\n",
    "test_5 = test_pre_tmp_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "previous-analysis",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  訓練データのコメントからページ数情報を抽出し格納\n",
    "l = train_pre_df[\"comments\"].values.tolist()\n",
    "tmp_com = []\n",
    "for i in l:\n",
    "    if \"pages\" in str(i):\n",
    "        pos = i.find(\"pages\")\n",
    "        txt = i[:pos]\n",
    "        txt = re.sub(r\"\\D\", \"\", txt)\n",
    "        tmp_com.append(txt)\n",
    "    else:\n",
    "        tmp_com.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "celtic-truck",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist = []\n",
    "for txt in tmp_com:\n",
    "    try:\n",
    "        num = int(txt)\n",
    "        numlist.append(num)\n",
    "    except ValueError:\n",
    "        numlist.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accredited-supervision",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist2 = []\n",
    "for num in numlist:\n",
    "    if num >= 1000:\n",
    "        numlist2.append(0)\n",
    "    else:\n",
    "        numlist2.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fantastic-russian",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp = np.array(numlist2)\n",
    "train_6 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "running-blink",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  テストデータのコメントからページ数情報を抽出し格納\n",
    "l = test_df[\"comments\"].values.tolist()\n",
    "tmp_com = []\n",
    "for i in l:\n",
    "    if \"pages\" in str(i):\n",
    "        pos = i.find(\"pages\")\n",
    "        txt = i[:pos]\n",
    "        txt = re.sub(r\"\\D\", \"\", txt)\n",
    "        tmp_com.append(txt)\n",
    "    else:\n",
    "        tmp_com.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "expired-employer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist = []\n",
    "for txt in tmp_com:\n",
    "    try:\n",
    "        num = int(txt)\n",
    "        numlist.append(num)\n",
    "    except ValueError:\n",
    "        numlist.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "selective-right",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist2 = []\n",
    "for num in numlist:\n",
    "    if num >= 1000:\n",
    "        numlist2.append(0)\n",
    "    else:\n",
    "        numlist2.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "compressed-filling",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp = np.array(numlist2)\n",
    "test_6 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "framed-flush",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  訓練データのコメントから図表情報を抽出し格納\n",
    "m = train_pre_df[\"comments\"].values.tolist()\n",
    "tmp_com = []\n",
    "for i in m:\n",
    "    if \"figures\" in str(i):\n",
    "        pos = i.find(\"figures\")\n",
    "        txt = i[:pos]\n",
    "        txt = re.sub(r\"\\D\", \"\", txt)\n",
    "        tmp_com.append(txt)\n",
    "    else:\n",
    "        tmp_com.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "colonial-aviation",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist = []\n",
    "for txt in tmp_com:\n",
    "    try:\n",
    "        num = int(txt)\n",
    "        numlist.append(num)\n",
    "    except ValueError:\n",
    "        numlist.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "linear-welcome",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist2 = []\n",
    "for num in numlist:\n",
    "    if num >= 10000:\n",
    "        numlist2.append(0)\n",
    "    else:\n",
    "        numlist2.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "personalized-bruce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp = np.array(numlist2)\n",
    "train_7 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unknown-tonight",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  テストデータのコメントから図表情報を抽出し格納\n",
    "m = test_df[\"comments\"].values.tolist()\n",
    "tmp_com = []\n",
    "for i in m:\n",
    "    if \"figures\" in str(i):\n",
    "        pos = i.find(\"figures\")\n",
    "        txt = i[:pos]\n",
    "        txt = re.sub(r\"\\D\", \"\", txt)\n",
    "        tmp_com.append(txt)\n",
    "    else:\n",
    "        tmp_com.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "excellent-peter",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist = []\n",
    "for txt in tmp_com:\n",
    "    try:\n",
    "        num = int(txt)\n",
    "        numlist.append(num)\n",
    "    except ValueError:\n",
    "        numlist.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "renewable-divide",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numlist2 = []\n",
    "for num in numlist:\n",
    "    if num >= 10000:\n",
    "        numlist2.append(0)\n",
    "    else:\n",
    "        numlist2.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bound-isaac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp = np.array(numlist2)\n",
    "test_7 = tmp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-warrior",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "opposite-dakota",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = np.concatenate([np.array(train), np.array(train_2), np.array(train_3), np.array(train_4), np.array(train_5), np.array(train_6), np.array(train_7), np.array(train_8), np.array(train_feat)], axis=1)\n",
    "target = np.array(np.log1p(target))\n",
    "test = np.concatenate([np.array(test), np.array(test_2), np.array(test_3), np.array(test_4), np.array(test_5), np.array(test_6), np.array(test_7), np.array(test_8), np.array(test_feat)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "south-period",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15117, 909)\n",
      "(15117,)\n",
      "(59084, 909)\n"
     ]
    }
   ],
   "source": [
    "# 各データのサイズの確認\n",
    "print(train.shape)\n",
    "print(target.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-paper",
   "metadata": {},
   "source": [
    "(15117, 610)\n",
    "(15117,)\n",
    "(59084, 610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "overhead-dancing",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################3\n",
    "### LGBで学習、予測する関数の定義\n",
    "########################################################\n",
    "def Train_and_Pred(train,target,test):\n",
    "    # --------------------------------------\n",
    "    # パラメータ定義\n",
    "    # --------------------------------------\n",
    "    lgb_params = {\n",
    "                    'objective': 'root_mean_squared_error',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'n_estimators': 50000,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 3,\n",
    "                    'reg_alpha': 8,\n",
    "                    'reg_lambda': 2,\n",
    "                    'random_state': SEED,\n",
    "                    \"bagging_fraction\": 0.8402379446262978,\n",
    "                    \"bagging_freq\": 4,\n",
    "                    \"feature_fraction\": 0.74623605968501,\n",
    "                    \"lambda_l1\": 0.01113869595673112,\n",
    "                    \"lambda_l2\": 8.706009358617911e-07,\n",
    "                    \"learning_rate\": 0.012307412937706345,\n",
    "                    \"min_child_samples\": 18,\n",
    "                    \"num_leaves\": 8,        \n",
    "                  }\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 学習と予測\n",
    "    # --------------------------------------\n",
    "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "    lgb_oof = np.zeros(train.shape[0])\n",
    "    lgb_pred = 0\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X=train)):\n",
    "        X_train, y_train = train[trn_idx], target[trn_idx]\n",
    "        X_valid, y_valid = train[val_idx], target[val_idx]\n",
    "        X_test = test\n",
    "\n",
    "        # LightGBM\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=(X_valid, y_valid),\n",
    "                  eval_metric='rmse',\n",
    "                  verbose=False,\n",
    "                  early_stopping_rounds=500\n",
    "                  )\n",
    "\n",
    "        lgb_oof[val_idx] = model.predict(X_valid)\n",
    "        lgb_pred += model.predict(X_test) / NFOLDS\n",
    "        rmsle = mean_squared_error(y_valid, lgb_oof[val_idx], squared=False)\n",
    "        print(f\"fold {fold} lgb score: {rmsle}\")\n",
    "\n",
    "    rmsle = mean_squared_error(target, lgb_oof, squared=False)\n",
    "    print(\"+-\" * 40)\n",
    "    print(f\"score: {rmsle}\")\n",
    "    print(f\"model score: {model.score(train, target)}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 提出ファイルの作成\n",
    "    # ------------------------------------------------------------------------------\n",
    "    test_predicted = np.expm1(lgb_pred)\n",
    "\n",
    "    submit_df = pd.DataFrame({'id': test_index})\n",
    "    submit_df['cites'] = np.where(test_predicted < 0, 0, test_predicted)\n",
    "    submit_df.to_csv(\"sub27.csv\", index=False)\n",
    "    blob = bucket.blob(\"sub27.csv\")\n",
    "    blob.upload_from_filename(filename=\"sub27.csv\")\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "differential-strain",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8402379446262978, subsample=0.5 will be ignored. Current value: bagging_fraction=0.8402379446262978\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74623605968501, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.74623605968501\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=3 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 0 lgb score: 0.5239869189669257\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8402379446262978, subsample=0.5 will be ignored. Current value: bagging_fraction=0.8402379446262978\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74623605968501, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.74623605968501\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=3 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 1 lgb score: 0.5013367799068057\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8402379446262978, subsample=0.5 will be ignored. Current value: bagging_fraction=0.8402379446262978\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74623605968501, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.74623605968501\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=3 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 2 lgb score: 0.5218132120825543\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8402379446262978, subsample=0.5 will be ignored. Current value: bagging_fraction=0.8402379446262978\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74623605968501, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.74623605968501\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=3 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 3 lgb score: 0.4910580967490293\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8402379446262978, subsample=0.5 will be ignored. Current value: bagging_fraction=0.8402379446262978\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74623605968501, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.74623605968501\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=3 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 4 lgb score: 0.501724314684782\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "score: 0.508145326437862\n",
      "model score: 0.8809856583254436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.508145326437862"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習と予測の実行\n",
    "Train_and_Pred(train, target, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "introductory-yahoo",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adolescent-rebel",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################3\n",
    "### catboostで学習、予測する関数の定義\n",
    "########################################################\n",
    "def Train_and_Pred_cat(train,target,test):\n",
    "    # --------------------------------------\n",
    "    # パラメータ定義\n",
    "    # --------------------------------------\n",
    "    cat_params = {\n",
    "                    'iterations': 5000,\n",
    "                    }\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 学習と予測\n",
    "    # --------------------------------------\n",
    "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "    cat_oof = np.zeros(train.shape[0])\n",
    "    cat_pred = 0\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X=train)):\n",
    "        X_train, y_train = train[trn_idx], target[trn_idx]\n",
    "        X_valid, y_valid = train[val_idx], target[val_idx]\n",
    "        X_test = test\n",
    "\n",
    "        # catboost\n",
    "        model = CatBoostRegressor(**cat_params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=(X_valid, y_valid),\n",
    "                  verbose=False,\n",
    "                  early_stopping_rounds=500\n",
    "                  )\n",
    "\n",
    "        cat_oof[val_idx] = model.predict(X_valid)\n",
    "        cat_pred += model.predict(X_test) / NFOLDS\n",
    "        rmsle = mean_squared_error(y_valid, cat_oof[val_idx], squared=False)\n",
    "        print(f\"fold {fold} cat score: {rmsle}\")\n",
    "\n",
    "    rmsle = mean_squared_error(target, cat_oof, squared=False)\n",
    "    print(\"+-\" * 40)\n",
    "    print(f\"score: {rmsle}\")\n",
    "    print(f\"model score: {model.score(train, target)}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 提出ファイルの作成\n",
    "    # ------------------------------------------------------------------------------\n",
    "    test_predicted = np.expm1(cat_pred)\n",
    "\n",
    "    submit_df = pd.DataFrame({'id': test_index})\n",
    "    submit_df['cites'] = np.where(test_predicted < 0, 0, test_predicted)\n",
    "    submit_df.to_csv(\"sub29.csv\", index=False)\n",
    "    blob = bucket.blob(\"sub29.csv\")\n",
    "    blob.upload_from_filename(filename=\"sub29.csv\")\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "attractive-happening",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 cat score: 0.525006069820722\n",
      "fold 1 cat score: 0.5004416056724608\n",
      "fold 2 cat score: 0.520675864495167\n",
      "fold 3 cat score: 0.48808685147541087\n",
      "fold 4 cat score: 0.498801607142225\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "score: 0.5067962994647837\n",
      "model score: 0.9044651176085674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5067962994647837"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習と予測の実行 cat\n",
    "Train_and_Pred_cat(train, target, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-employer",
   "metadata": {},
   "source": [
    "## score memo\n",
    "score: 0.5091439804187227\n",
    "model score: 0.8776748976366855\n",
    "\n",
    "score: 0.5349412183809286\n",
    "model score: 0.8561340308642265\n",
    "\n",
    "score: 0.510865780648061 -> PB:0.512208\n",
    "model score: 0.8613939830797712\n",
    "\n",
    "score: 0.5083981751554011\n",
    "model score: 0.8801454028408289\n",
    "\n",
    "#catboost params_non iter:5000\n",
    "score: 0.5089691056151895\n",
    "model score: 0.9088950014953863\n",
    "\n",
    "#catboost params_mix\n",
    "score: 0.5141818558985992\n",
    "model score: 0.8414627399770431\n",
    "score: 0.5104255055887893\n",
    "model score: 0.8661647531047596\n",
    "\n",
    "score: 0.5134752086890247\n",
    "model score: 0.8708150081478201\n",
    "\n",
    "#catboost params_non iter:5000 +figures info 1000>cut\n",
    "score: 0.508860847861468\n",
    "model score: 0.9071390413198503\n",
    "\n",
    "#catboost params_non iter:5000 +figures info 10000>cut\n",
    "score: 0.5084451492511196\n",
    "model score: 0.8748347769753742\n",
    "\n",
    "#LGBM ↑\n",
    "score: 0.5084060348263081\n",
    "model score: 0.873635256762187\n",
    "\n",
    "#LGBM +auth\n",
    "score: 0.508145326437862\n",
    "model score: 0.8809856583254436\n",
    "\n",
    "#cat +auth\n",
    "score: 0.5082768701494705\n",
    "model score: 0.8713432729258084\n",
    "\n",
    "#catboost params_non iter:5000 +figures info 10000>cut +abstract max\n",
    "score: 0.5067962994647837\n",
    "model score: 0.9044651176085674"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-parliament",
   "metadata": {},
   "source": [
    "## パラメーターの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "major-enzyme",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "measured-student",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.2, random_state=1234, shuffle=False, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "prime-consequence",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################3\n",
    "### LGBのパラメーター最適化\n",
    "########################################################\n",
    "def objective(trial):\n",
    "    # --------------------------------------\n",
    "    # パラメータ定義\n",
    "    # --------------------------------------\n",
    "    lgb_params = {\n",
    "                    'objective': 'root_mean_squared_error',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'n_estimators': 1000,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 3,\n",
    "                    'reg_alpha': 8,\n",
    "                    'reg_lambda': 2,\n",
    "                    'random_state': SEED,\n",
    "                    \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\",0.4,0.9),\n",
    "                    \"bagging_freq\": trial.suggest_int(\"bagging_freq\",1,10),\n",
    "                    \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\",0.4,0.9),\n",
    "                    \"lambda_l1\": 0.01113869595673112,\n",
    "                    \"lambda_l2\": 8.706009358617911e-07,\n",
    "                    \"learning_rate\": 0.012307412937706345,\n",
    "                    \"min_child_samples\": 18,\n",
    "                    \"num_leaves\": 8,        \n",
    "                  }\n",
    "\n",
    "    # --------------------------------------\n",
    "    # モデル構築\n",
    "    # --------------------------------------\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "    \n",
    "    model_lgb = lgb.train(lgb_params, lgb_train,\n",
    "                         valid_sets=lgb_eval,\n",
    "                         num_boost_round=100,\n",
    "                         early_stopping_rounds=20,\n",
    "                         verbose_eval=10,)\n",
    "    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n",
    "    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "hawaiian-arcade",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################3\n",
    "### catboostのパラメーター最適化\n",
    "########################################################\n",
    "def objective_cat(trial):\n",
    "    # --------------------------------------\n",
    "    # パラメータ定義\n",
    "    # --------------------------------------\n",
    "    cat_params = {\n",
    "                    'iterations' : 5000,                         \n",
    "                    'depth' : trial.suggest_int('depth', 4, 10),                                       \n",
    "                    'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 0.3),               \n",
    "                    'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "                    'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "                    'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "                    'od_wait' :trial.suggest_int('od_wait', 10, 50)       \n",
    "                  }\n",
    "\n",
    "    # --------------------------------------\n",
    "    # モデル構築\n",
    "    # --------------------------------------\n",
    "    \n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    test_pool = Pool(X_valid, y_valid)\n",
    "    \n",
    "    # 学習\n",
    "    model = CatBoostRegressor(**cat_params)\n",
    "    model.fit(train_pool)\n",
    "    # 予測\n",
    "    preds = model.predict(test_pool)\n",
    "    pred_labels = np.rint(preds)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-learning",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n",
    "study.optimize(objective, n_trials=50)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-arctic",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "{'bagging_fraction': 0.8457772186128385,\n",
    " 'bagging_freq': 1,\n",
    " 'feature_fraction': 0.8232043362355639}\n",
    " \n",
    " {'bagging_fraction': 0.8402379446262978,\n",
    " 'bagging_freq': 4,\n",
    " 'feature_fraction': 0.74623605968501}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-mouse",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n",
    "study.optimize(objective_cat, n_trials=50)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-french",
   "metadata": {},
   "source": [
    "{'iterations': 187,\n",
    " 'depth': 5,\n",
    " 'learning_rate': 0.10719804484767982,\n",
    " 'random_strength': 3,\n",
    " 'bagging_temperature': 0.4789624637920164,\n",
    " 'od_type': 'IncToDec',\n",
    " 'od_wait': 13}\n",
    " \n",
    " {'iterations': 746,\n",
    " 'depth': 4,\n",
    " 'learning_rate': 0.08182456890976228,\n",
    " 'random_strength': 18,\n",
    " 'bagging_temperature': 32.64440528937261,\n",
    " 'od_type': 'Iter',\n",
    " 'od_wait': 21}\n",
    " \n",
    " Trial 8 finished with value: 0.4876234943471408 and parameters: {'depth': 4, 'learning_rate': 0.010659941309425505, 'random_strength': 53, 'bagging_temperature': 0.03969167884674241, 'od_type': 'IncToDec', 'od_wait': 27}. Best is trial 8 with value: 0.4876234943471408."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-02-02-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
